import streamlit as st
import os
import json
from io import StringIO
import numpy as np
from typing import List, Dict
import re
import faiss
from sentence_transformers import SentenceTransformer

# Try to get OpenAI API key
try:
    openai_api_key = st.secrets["OPENAI_API_KEY"]
except:
    openai_api_key = None

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    st.warning("OpenAI not installed. Using mock LLM responses.")

# Initialize embedding model
@st.cache_resource
def get_embedding_model():
    """Load sentence transformer model (cached)"""
    return SentenceTransformer('all-MiniLM-L6-v2')

def create_faiss_index(dimension=384):
    """Create a new FAISS index for semantic search"""
    return faiss.IndexFlatIP(dimension)  # Inner product (cosine similarity)

def embed_texts(texts: List[str]) -> np.ndarray:
    """Embed texts using sentence-transformers"""
    model = get_embedding_model()
    embeddings = model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)
    return embeddings

def search_faiss_index(query: str, embeddings: np.ndarray, documents: List[Dict], top_k: int = 3) -> List[Dict]:
    """Search FAISS index for similar documents"""
    if len(documents) == 0:
        return []
    
    # Embed query
    query_embedding = embed_texts([query])
    
    # Create temporary FAISS index
    index = create_faiss_index(embeddings.shape[1])
    index.add(embeddings.astype('float32'))
    
    # Search
    scores, indices = index.search(query_embedding.astype('float32'), min(top_k, len(documents)))
    
    # Return results
    results = []
    for i, score in zip(indices[0], scores[0]):
        if score > 0:  # Only return relevant results
            results.append(documents[i])
    
    return results

def chunk_document(content: str, chunk_size: int = 500) -> List[str]:
    """Split document into chunks for better retrieval granularity"""
    # Simple sentence-based chunking
    sentences = re.split(r'[.!?]+', content)
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) < 10:  # Skip very short sentences
            continue
            
        # If adding this sentence would exceed chunk size, start new chunk
        if len(current_chunk) + len(sentence) > chunk_size and current_chunk:
            chunks.append(current_chunk.strip())
            current_chunk = sentence
        else:
            current_chunk += ". " + sentence if current_chunk else sentence
    
    # Add the last chunk if it exists
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    return [chunk for chunk in chunks if len(chunk.strip()) > 50]  # Filter very short chunks

def generate_hypotheses_with_llm(query: str, context_texts: List[str]) -> List[Dict]:
    """Generate RCA hypotheses using LLM with retrieved context"""
    context_str = "\n\n".join([f"Context {i+1}: {text}" for i, text in enumerate(context_texts)])
    
    prompt = f"""Based on the following context and the RCA query, generate 3-5 potential root cause hypotheses.

Query: {query}

Relevant Context:
{context_str}

For each hypothesis, provide:
1. A clear hypothesis statement
2. Likelihood rating (High/Medium/Low)
3. Supporting evidence from the context
4. Next steps to validate the hypothesis

Format as JSON array with fields: hypothesis, likelihood, evidence, next_steps"""
    
    if OPENAI_AVAILABLE and openai_api_key:
        try:
            client = openai.OpenAI(api_key=openai_api_key)
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are an expert in root cause analysis. Provide structured hypotheses based on the given context."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7
            )
            
            # Try to parse JSON response
            try:
                hypotheses = json.loads(response.choices[0].message.content)
                if isinstance(hypotheses, list):
                    return hypotheses
            except json.JSONDecodeError:
                pass
                
        except Exception as e:
            st.error(f"OpenAI API error: {str(e)}")
    
    # Fallback: Mock hypotheses based on context keywords
    mock_hypotheses = [
        {
            "hypothesis": "Process inefficiency or bottleneck",
            "likelihood": "High",
            "evidence": "Context suggests workflow or process-related issues",
            "next_steps": "Analyze process metrics and identify bottlenecks"
        },
        {
            "hypothesis": "Resource or capacity constraints",
            "likelihood": "Medium",
            "evidence": "Indicators of resource limitations in the context",
            "next_steps": "Review resource allocation and capacity planning"
        },
        {
            "hypothesis": "System or technical failure",
            "likelihood": "Medium",
            "evidence": "Technical indicators present in the context",
            "next_steps": "Investigate system logs and technical infrastructure"
        }
    ]
    
    return mock_hypotheses

# === STREAMLIT UI ===

if st.sidebar.button("Full Session RESET"):
for key in list(st.session_state.keys()):
    del st.session_state[key]
st.experimental_rerun()

st.set_page_config(page_title="Document Q&A with RCA", page_icon="üìä", layout="wide")
st.title("üìä Document Q&A with Root Cause Analysis")

# Initialize session state
if 'knowledge_docs' not in st.session_state:
    st.session_state.knowledge_docs = {}
if 'document_embeddings' not in st.session_state:
    st.session_state.document_embeddings = np.array([])
if 'document_chunks' not in st.session_state:
    st.session_state.document_chunks = []

# === DOCUMENT UPLOAD SECTION ===
st.header("üìÑ Knowledge Base Management")

with st.expander("Upload Documents", expanded=True):
    uploaded_files = st.file_uploader(
        "Choose files to add to knowledge base",
        accept_multiple_files=True,
        type=['txt', 'md', 'py', 'json', 'csv']
    )
    
    if uploaded_files:
        for uploaded_file in uploaded_files:
            file_id = uploaded_file.name
            
            # Skip if already processed
            if file_id in st.session_state.knowledge_docs:
                continue
            
            # Read and process file
            content = StringIO(uploaded_file.getvalue().decode("utf-8")).read()
            
            # Chunk the document
            chunks = chunk_document(content)
            
            if chunks:
                with st.spinner(f"Processing {file_id}..."):
                    # Embed chunks
                    chunk_embeddings = embed_texts(chunks)
                    
                    # Store document info
                    st.session_state.knowledge_docs[file_id] = {
                        'content': content,
                        'chunk_count': len(chunks),
                        'file_size': len(content)
                    }
                    
                    # Add to document chunks and embeddings
                    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):
                        st.session_state.document_chunks.append({
                            'doc_id': file_id,
                            'chunk_id': f"{file_id}_chunk_{i}",
                            'content': chunk
                        })
                    
                    # Update embeddings array
                    if st.session_state.document_embeddings.size == 0:
                        st.session_state.document_embeddings = chunk_embeddings
                    else:
                        st.session_state.document_embeddings = np.vstack([
                            st.session_state.document_embeddings, 
                            chunk_embeddings
                        ])
                
                st.success(f"‚úÖ Processed {file_id}: {len(chunks)} chunks")
        
        st.rerun()

# Display current knowledge base
if st.session_state.knowledge_docs:
    st.subheader("üìö Current Knowledge Base")
    for doc_id, doc_info in st.session_state.knowledge_docs.items():
        col1, col2, col3 = st.columns([3, 1, 1])
        with col1:
            st.write(f"üìÑ **{doc_id}**")
        with col2:
            st.write(f"{doc_info['chunk_count']} chunks")
        with col3:
            st.write(f"{doc_info['file_size']:,} chars")

# === RCA QUERY SECTION ===
st.header("üîç Root Cause Analysis Query")

if st.session_state.knowledge_docs:
    rca_query = st.text_area(
        "Enter your RCA question:",
        placeholder="e.g., Why did sales drop in Q3? What caused the system outage? Why is customer satisfaction declining?",
        height=100
    )
    
    if st.button("üéØ Analyze Root Causes", type="primary") and rca_query:
        with st.spinner("Analyzing and generating hypotheses..."):
            # === RETRIEVAL STEP ===
            st.subheader("üîé Document Retrieval")
            
            # Search for relevant chunks using FAISS
            relevant_chunks = search_faiss_index(
                rca_query, 
                st.session_state.document_embeddings, 
                st.session_state.document_chunks, 
                top_k=3
            )
            
            if relevant_chunks:
                st.subheader("üìã Retrieved Context")
                context_texts = []
                
                for i, chunk_info in enumerate(relevant_chunks):
                    with st.expander(f"Context {i+1} (from {chunk_info['doc_id']})"):
                        st.write(chunk_info['content'])
                    context_texts.append(chunk_info['content'])
                
                # === LLM GENERATION STEP ===
                st.subheader("üéØ Generated RCA Hypotheses")
                
                # Generate hypotheses using LLM with retrieved context
                hypotheses = generate_hypotheses_with_llm(rca_query, context_texts)
                
                # Display hypotheses in the same format as before
                for i, hypothesis in enumerate(hypotheses, 1):
                    with st.expander(f"Hypothesis {i}: {hypothesis['hypothesis']}"):
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.write(f"**Likelihood:** {hypothesis['likelihood']}")
                            st.write(f"**Evidence:** {hypothesis['evidence']}")
                        
                        with col2:
                            st.write(f"**Next Steps:** {hypothesis['next_steps']}")
            else:
                st.warning("No relevant context found in knowledge store. Try adding more documents or refining your query.")
else:
    st.info("Please add documents to the knowledge store before performing RCA queries.")

# === SYSTEM INFO ===
with st.sidebar:
    st.header("System Information")
    st.write(f"**Documents in store:** {len(st.session_state.knowledge_docs)}")
    st.write(f"**Total chunks:** {len(st.session_state.document_chunks)}")
    
    if st.button("Clear Knowledge Store"):
        st.session_state.knowledge_docs = {}
        st.session_state.document_embeddings = np.array([])
        st.session_state.document_chunks = []
        st.success("Knowledge store cleared!")
        st.rerun()

    
    # Technical notes
    with st.expander("üîß Technical Notes"):
        st.write("""
        **Current Implementation:**
        - FAISS vector search with cosine similarity
        - Sentence-transformers (all-MiniLM-L6-v2) for embeddings  
        - Semantic search and retrieval
        - OpenAI integration for hypothesis generation
        - Session-based storage (arrays only, no object refs)
        
        **Features:**
        - Real semantic similarity matching
        - Efficient vector search with FAISS
        - Proper document chunking and embedding
        - Production-ready vector storage
        """)
